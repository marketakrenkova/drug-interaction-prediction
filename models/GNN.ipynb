{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23348f7e",
   "metadata": {},
   "source": [
    "# Graph Neural Network for DDI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e5059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "from ogb.linkproppred import Evaluator, PygLinkPropPredDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19138554",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/dataset-ogb/' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131415e7",
   "metadata": {},
   "source": [
    "### Load OGB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi', root=dataset_dir, transform=T.ToSparseTensor())\n",
    "    \n",
    "split_edge = dataset.get_edge_split()\n",
    "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc962ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.emb = emb\n",
    "\n",
    "        assert (self.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.Dropout(self.dropout),\n",
    "            torch.nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()    \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        # Return node embeddings after post-message passing if specified\n",
    "        if self.emb:\n",
    "            return x\n",
    "\n",
    "        # Else return class probabilities for each node\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, predictor, x, adj_t, split_edge, optimizer, batch_size):\n",
    "    row, col, _ = adj_t.coo()\n",
    "    edge_index = torch.stack([col, row], dim=0)\n",
    "\n",
    "    model.train()\n",
    "    predictor.train()\n",
    "\n",
    "    pos_train_edge = split_edge['train']['edge'].to(x.device)\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h = model(x, adj_t)\n",
    "\n",
    "        edge = pos_train_edge[perm].t()\n",
    "\n",
    "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
    "\n",
    "        edge = negative_sampling(edge_index, num_nodes=x.size(0),\n",
    "                                 num_neg_samples=perm.size(0), method='dense')\n",
    "\n",
    "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(x, 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        num_examples = pos_out.size(0)\n",
    "        total_loss += loss.item() * num_examples\n",
    "        total_examples += num_examples\n",
    "\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, predictor, x, adj_t, split_edge, evaluator, batch_size):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    h = model(x, adj_t)\n",
    "\n",
    "    pos_train_edge = split_edge['eval_train']['edge'].to(x.device)\n",
    "    pos_valid_edge = split_edge['valid']['edge'].to(x.device)\n",
    "    neg_valid_edge = split_edge['valid']['edge_neg'].to(x.device)\n",
    "\n",
    "    pos_train_preds = []\n",
    "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n",
    "        edge = pos_train_edge[perm].t()\n",
    "        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n",
    "\n",
    "    pos_valid_preds = []\n",
    "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
    "        edge = pos_valid_edge[perm].t()\n",
    "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
    "\n",
    "    neg_valid_preds = []\n",
    "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
    "        edge = neg_valid_edge[perm].t()\n",
    "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
    "\n",
    "    results = {}\n",
    "    for K in [10, 20, 30]:\n",
    "        evaluator.K = K\n",
    "        train_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_train_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        valid_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_valid_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        results[f'Hits@{K}'] = (train_hits, valid_hits)\n",
    "\n",
    "    return results\n",
    "\n",
    "# testing on test data\n",
    "@torch.no_grad()\n",
    "def test_metrics(model, predictor, x, adj_t, split_edge, evaluator, batch_size):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    h = model(x, adj_t)\n",
    "\n",
    "    pos_test_edge = split_edge['test']['edge'].to(x.device)\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].to(x.device)\n",
    "\n",
    "    pos_test_preds = []\n",
    "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
    "        edge = pos_test_edge[perm].t()\n",
    "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
    "\n",
    "    neg_test_preds = []\n",
    "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
    "        edge = neg_test_edge[perm].t()\n",
    "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
    "\n",
    "    results = {}\n",
    "    for K in [10, 20, 30]:\n",
    "        evaluator.K = K\n",
    "        test_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_test_pred,\n",
    "            'y_pred_neg': neg_test_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        results[f'Hits@{K}'] = test_hits\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(parameters, dataset, device, gnn_dir):\n",
    "    log_steps = 1\n",
    "\n",
    "    data = dataset[0]\n",
    "    adj_t = data.adj_t.to(device)\n",
    "    split_edge = dataset.get_edge_split()\n",
    "\n",
    "    evaluator = Evaluator(name='ogbl-ddi')\n",
    "\n",
    "    # We randomly pick some training samples that we want to evaluate on:\n",
    "    torch.manual_seed(12345)\n",
    "    idx = torch.randperm(split_edge['train']['edge'].size(0))\n",
    "    idx = idx[:split_edge['valid']['edge'].size(0)]\n",
    "    split_edge['eval_train'] = {'edge': split_edge['train']['edge'][idx]}\n",
    "\n",
    "    emb = torch.nn.Embedding(data.adj_t.size(0), parameters['nodes_emb']).to(device)\n",
    "    predictor = LinkPredictor(parameters['hidden_channels'], parameters['hidden_channels'], 1, parameters['num_layers'], parameters['dropout']).to(device)\n",
    "\n",
    "    model = GNNStack(parameters['nodes_emb'], parameters['hidden_channels'], parameters['hidden_channels'], parameters['num_layers'], parameters['dropout']).to(device)\n",
    "\n",
    "    print('Number of parameters:',\n",
    "            sum(p.numel() for p in list(model.parameters()) +\n",
    "            list(predictor.parameters()) + list(emb.parameters())))\n",
    "\n",
    "    history = {'loss': [], 'hits@20': []}\n",
    "    \n",
    "    if os.path.exists(gnn_dir):\n",
    "        model.load_state_dict(torch.load(gnn_dir + 'model_weights.pth'))\n",
    "        predictor.load_state_dict(torch.load(gnn_dir + 'predictor_weights.pth'))\n",
    "        emb.load_state_dict(torch.load(gnn_dir + 'embedding_weights.pth'))\n",
    "    else:\n",
    "        print('Training...')\n",
    "        torch.nn.init.xavier_uniform_(emb.weight)\n",
    "        model.reset_parameters()\n",
    "        predictor.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            list(model.parameters()) + list(emb.parameters()) +\n",
    "            list(predictor.parameters()), lr=parameters['lr'])\n",
    "\n",
    "        for epoch in range(1, 1 + parameters['num_epochs']):\n",
    "            loss = train(model, predictor, emb.weight, adj_t, split_edge, optimizer, parameters['batch_size'])\n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            results = test(model, predictor, emb.weight, adj_t, split_edge, evaluator, parameters['batch_size'])            \n",
    "            history['hits@20'].append(results['Hits@20'])\n",
    "\n",
    "            if epoch % log_steps == 0:\n",
    "                for key, result in results.items():\n",
    "                    train_hits, valid_hits = result\n",
    "                    print(key)\n",
    "                    print(f'Epoch: {epoch:02d}, '\n",
    "                        f'Loss: {loss:.4f}, '\n",
    "                        f'Train: {100 * train_hits:.2f}%, '\n",
    "                        f'Valid: {100 * valid_hits:.2f}% '\n",
    "                    )\n",
    "                print('---')\n",
    "\n",
    "        print('Training done.\\n') \n",
    "\n",
    "    print('Results on test data...')\n",
    "    test_result = test_metrics(model, predictor, emb.weight, adj_t, split_edge, evaluator, parameters['batch_size'])\n",
    "    for key, result in test_result.items():\n",
    "        test_hits = result\n",
    "        print(key)\n",
    "        print(f'Test: {100 * test_hits:.2f}%')\n",
    "    print('---')\n",
    "\n",
    "    if not os.path.exists(gnn_dir):\n",
    "        os.mkdir(gnn_dir)\n",
    "        torch.save(model.state_dict(), gnn_dir + 'model_weights.pth')\n",
    "        torch.save(predictor.state_dict(), gnn_dir + 'predictor_weights.pth')\n",
    "        torch.save(emb.state_dict(), gnn_dir + 'embedding_weights.pth')\n",
    "\n",
    "    return history      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a20269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, num_epochs, experiment, gnn_results_dir):\n",
    "    hits_val = [x[1] for x in history['hits@20']]\n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    fig = plt.figure() # creates new figure each time?\n",
    "    plt.plot(epochs, history['loss'], label = \"training loss\")\n",
    "    plt.plot(epochs, hits_val, label = \"hits@20 on validation data\")\n",
    "  \n",
    "    plt.xticks([e for e in epochs if e % 5 == 0])\n",
    "    plt.legend()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.savefig(gnn_results_dir + 'gnn_experiment-' + str(experiment) +'.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f201a3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# load special configuration\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#     with open('../experiments/benchmark-ogb/gnn-experiment-' + str(experiment) + '/config.json', 'r') as f:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#         config = json.load(f)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# run experiment\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     gnn_results_dir = '../experiments/benchmark-ogb/gnn-experiment-'+ str(experiment) + '/results/'\u001b[39;00m\n\u001b[1;32m     27\u001b[0m gnn_results_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/gnn_results/gnn2/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m(config, dataset, device, gnn_results_dir)\n\u001b[1;32m     29\u001b[0m plot_history(history, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m], experiment, gnn_results_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../data/dataset-ogb'\n",
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi', root=dataset_dir, transform=T.ToSparseTensor())\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "\n",
    "# default configuration\n",
    "config = dict(\n",
    "    hidden_channels = 96,\n",
    "    nodes_emb = 256,\n",
    "    num_layers = 2,\n",
    "    dropout = 0.2,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 1024,\n",
    ")\n",
    "\n",
    "    # load special configuration\n",
    "#     with open('../experiments/benchmark-ogb/gnn-experiment-' + str(experiment) + '/config.json', 'r') as f:\n",
    "#         config = json.load(f)\n",
    "\n",
    "    # run experiment\n",
    "#     gnn_results_dir = '../experiments/benchmark-ogb/gnn-experiment-'+ str(experiment) + '/results/'\n",
    "gnn_results_dir = 'results/gnn_results/gnn2/'\n",
    "history = run(config, dataset, device, gnn_results_dir)\n",
    "plot_history(history, config['num_epochs'], experiment, gnn_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc91a9",
   "metadata": {},
   "source": [
    "## My GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60130690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888313b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB00091</td>\n",
       "      <td>interacts</td>\n",
       "      <td>DB00680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB11730</td>\n",
       "      <td>interacts</td>\n",
       "      <td>DB01229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB00674</td>\n",
       "      <td>interacts</td>\n",
       "      <td>DB09083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB01189</td>\n",
       "      <td>interacts</td>\n",
       "      <td>DB08936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB00656</td>\n",
       "      <td>interacts</td>\n",
       "      <td>DB01193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      head   relation     tail\n",
       "0  DB00091  interacts  DB00680\n",
       "1  DB11730  interacts  DB01229\n",
       "2  DB00674  interacts  DB09083\n",
       "3  DB01189  interacts  DB08936\n",
       "4  DB00656  interacts  DB01193"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddi = pd.read_csv('../data/triplets/train.tsv', sep='\\t')\n",
    "ddi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a570a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drug_id</th>\n",
       "      <th>mappedID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000160-96-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001415-66-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10025-69-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10025-70-4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10025-77-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33041</th>\n",
       "      <td>sideeffect_9967</td>\n",
       "      <td>33041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33042</th>\n",
       "      <td>sideeffect_9968</td>\n",
       "      <td>33042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33043</th>\n",
       "      <td>sideeffect_998</td>\n",
       "      <td>33043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33044</th>\n",
       "      <td>sideeffect_999</td>\n",
       "      <td>33044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33045</th>\n",
       "      <td>vitamin supplements</td>\n",
       "      <td>33045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33046 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   drug_id  mappedID\n",
       "0             1000160-96-2         0\n",
       "1             1001415-66-2         1\n",
       "2               10025-69-1         2\n",
       "3               10025-70-4         3\n",
       "4               10025-77-1         4\n",
       "...                    ...       ...\n",
       "33041      sideeffect_9967     33041\n",
       "33042      sideeffect_9968     33042\n",
       "33043       sideeffect_998     33043\n",
       "33044       sideeffect_999     33044\n",
       "33045  vitamin supplements     33045\n",
       "\n",
       "[33046 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_drug_id = np.unique(ddi[['head', 'tail']].values)\n",
    "unique_drug_id = pd.DataFrame(data={\n",
    "    'drug_id': unique_drug_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_drug_id)),\n",
    "})\n",
    "unique_drug_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f348361",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = torch.tensor(unique_drug_id.mappedID.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1716ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2061, 4365, 2602,  ..., 9643, 9643, 9643],\n",
       "        [2608, 3131, 3859,  ..., 8586, 8053, 8210]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_ids = pd.merge(ddi['head'], unique_drug_id,\n",
    "                            left_on='head', right_on='drug_id', how='left')\n",
    "head_ids = torch.from_numpy(head_ids['mappedID'].values)\n",
    "\n",
    "tail_ids = pd.merge(ddi['tail'], unique_drug_id,\n",
    "                            left_on='tail', right_on='drug_id', how='left')\n",
    "tail_ids = torch.from_numpy(tail_ids['mappedID'].values)\n",
    "\n",
    "edge_index = torch.stack([head_ids, tail_ids], dim=0)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b85f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader, DataLoader\n",
    "\n",
    "data = Data(x=nodes, edge_index=edge_index)\n",
    "# data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f5a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "tensor([[128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "         142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "         156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "         170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "         184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "         198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "         212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "         226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "         240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "         254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267,\n",
      "         268, 262, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
      "         281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
      "         295, 296, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "         302, 318, 322, 323, 324, 325, 326, 327, 328, 329, 318, 330, 298, 331,\n",
      "         332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345,\n",
      "         346, 347, 348, 349, 350, 351, 352, 353, 354, 319, 355, 356, 301, 302,\n",
      "         321, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369,\n",
      "         370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n",
      "         272, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 356, 394, 395,\n",
      "         396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
      "         410, 411, 412, 413],\n",
      "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "         168, 169, 170, 171, 171, 171, 171, 171, 171, 172, 173, 174, 175, 176,\n",
      "         177, 178, 179, 180, 181, 182, 183, 183, 184, 184, 184, 184, 184, 184,\n",
      "         185, 185, 185, 185, 185, 185, 186, 186, 186, 186, 186, 186, 187, 187,\n",
      "         188, 189, 190, 191, 192, 192, 192, 192, 192, 192, 193, 193, 193, 193,\n",
      "         193, 193, 194, 195, 196, 197, 198, 199, 200, 201, 201, 201, 201, 201,\n",
      "         201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214,\n",
      "         215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,\n",
      "         229, 230, 231, 232, 233, 234, 235, 236, 236, 236, 236, 236, 236, 237,\n",
      "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "         252, 253, 254, 255]])\n"
     ]
    }
   ],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 30 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[6] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    "    input_nodes=None,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(train_loader))\n",
    "print(sampled_data.batch_size)\n",
    "print(sampled_data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a19a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# num_epochs = 10\n",
    "# model_cora, train_losses, val_losses = train(dataset_cora, device, num_epochs)\n",
    "# plot_losses(train_losses, val_losses, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "586dcf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2286393"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd45eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    hidden_channels = 96,\n",
    "    nodes_emb = 256,\n",
    "    num_layers = 2,\n",
    "    dropout = 0.2,\n",
    "    lr = 0.001,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a2d4ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.18 GiB (GPU 0; 7.79 GiB total capacity; 5.20 GiB already allocated; 535.94 MiB free; 6.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnodes_emb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m predictor \u001b[38;5;241m=\u001b[39m LinkPredictor(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GNNStack(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes_emb\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.18 GiB (GPU 0; 7.79 GiB total capacity; 5.20 GiB already allocated; 535.94 MiB free; 6.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "emb = torch.nn.Embedding(data.edge_index.size(1), config['nodes_emb']).to(device)\n",
    "predictor = LinkPredictor(config['hidden_channels'], config['hidden_channels'], 1, config['num_layers'], config['dropout']).to(device)\n",
    "model = GNNStack(config['nodes_emb'], config['hidden_channels'], config['hidden_channels'], config['num_layers'], config['dropout']).to(device)\n",
    "    \n",
    "torch.nn.init.xavier_uniform_(emb.weight)\n",
    "model.reset_parameters()\n",
    "predictor.reset_parameters()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(emb.parameters()) +\n",
    "    list(predictor.parameters()), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d220be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for sample in tqdm.tqdm(train_loader):\n",
    "        sample.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h = model(emb.weight, sample.edge_index)\n",
    "\n",
    "        edge = sample.edge_index\n",
    "\n",
    "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
    "\n",
    "        edge = negative_sampling(sample.edge_index, num_nodes=sample.x.size(0),\n",
    "                                 num_neg_samples=sample.size(0), method='dense')\n",
    "\n",
    "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
    "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        print(loss.values)\n",
    "        loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bbe77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

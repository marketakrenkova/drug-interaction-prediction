{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph\n",
    "#### Drug-Food or Drug-Supplements interaction prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pykeen.predict import predict_target\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory, CoreTriplesFactory\n",
    "from pykeen.models import TransE\n",
    "from pykeen.evaluation import RankBasedEvaluator, OGBEvaluator\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "from kg_model import KG_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/triplets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_df = pd.read_csv(data_dir + 'ddi.tsv', sep='\\t', index_col=[0])\n",
    "ddi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total interactions:', ddi_df.shape[0])\n",
    "print('unique interactions:', len(set(ddi_df.interaction)))\n",
    "\n",
    "interaction_counts = ddi_df.groupby(by=['interaction']).size()\n",
    "interaction_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ddi_df.interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_supplement_df = pd.read_csv(data_dir + 'ds_relations.tsv', sep='\\t', index_col=[0])\n",
    "# drug_supplement_df = drug_supplement_df[drug_supplement_df['REL'] != 'has_ingredient']\n",
    "drug_supplement_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total interactions:', drug_supplement_df.shape[0])\n",
    "print('unique interactions:', len(set(drug_supplement_df.REL)))\n",
    "\n",
    "ds_interaction_counts = drug_supplement_df.groupby(by=['REL']).size()\n",
    "ds_interaction_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_count = pd.concat([interaction_counts, ds_interaction_counts])\n",
    "interactions_count.to_csv('interaction_counts.csv', header=['interaction_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_size(n):\n",
    "    if n == 2:\n",
    "        return 1, 1\n",
    "    if n == 3:\n",
    "        return 1, 2\n",
    "    if n == 4:\n",
    "        return 2, 3\n",
    "    if n == 5:\n",
    "        return 3, 4\n",
    "    if n == 6:\n",
    "        return 4, 5\n",
    "    # n == 7\n",
    "    return 4, 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : valid : test = 80 : 10 : 10\n",
    "def split_data_relation(df_relation):\n",
    "    \n",
    "    # too few triplets with the realtion\n",
    "    if df_relation.shape[0] <= 7:\n",
    "        train_size, valid_size = compute_size(df_relation.shape[0])\n",
    "        \n",
    "        # shuffle df_relation\n",
    "        df_relation = df_relation.sample(frac=1, random_state=42)\n",
    "        \n",
    "        X_train = df_relation.iloc[:train_size]\n",
    "        X_valid = df_relation.iloc[train_size:valid_size]\n",
    "        X_test = df_relation.iloc[valid_size:]\n",
    "\n",
    "    else:\n",
    "        X_train, X_rem = train_test_split(df_relation, train_size=0.8, random_state=42)\n",
    "        X_valid, X_test = train_test_split(X_rem, test_size=0.5, random_state=42)\n",
    "        \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split drug-supplements relation dataset\n",
    "def split_drug_supplements_dataset(drug_supplement_df):\n",
    "    relations = set(drug_supplement_df.REL)\n",
    "    \n",
    "    train_triplets = pd.DataFrame(columns=['CUI1', 'REL', 'CUI2'])\n",
    "    valid_triplets = pd.DataFrame(columns=['CUI1', 'REL', 'CUI2'])\n",
    "    test_triplets = pd.DataFrame(columns=['CUI1', 'REL', 'CUI2'])\n",
    "\n",
    "    for rel in relations:\n",
    "        train, valid, test = split_data_relation(drug_supplement_df[drug_supplement_df['REL'] == rel])\n",
    "        train_triplets = pd.concat([train_triplets, train])\n",
    "        valid_triplets = pd.concat([valid_triplets, valid])\n",
    "        test_triplets = pd.concat([test_triplets, test])\n",
    "\n",
    "    train_triplets.rename(columns={'CUI1': 'head', 'REL': 'relation', 'CUI2': 'tail'}, inplace=True)\n",
    "    valid_triplets.rename(columns={'CUI1': 'head', 'REL': 'relation', 'CUI2': 'tail'}, inplace=True)\n",
    "    test_triplets.rename(columns={'CUI1': 'head', 'REL': 'relation', 'CUI2': 'tail'}, inplace=True)    \n",
    "\n",
    "\n",
    "    print('train dataset size:', train_triplets.shape[0])\n",
    "    print('validation dataset size:',valid_triplets.shape[0])\n",
    "    print('test dataset size:',test_triplets.shape[0])\n",
    "    \n",
    "    return train_triplets, valid_triplets, test_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split drug-drug interaction dataset (from DrugBank)\n",
    "def split_ddi_dataset(ddi_df):\n",
    "    interactions = set(ddi_df.interaction)\n",
    "    \n",
    "    train_triplets = pd.DataFrame(columns=['drug1', 'interaction', 'drug2'])\n",
    "    valid_triplets = pd.DataFrame(columns=['drug1', 'interaction', 'drug2'])\n",
    "    test_triplets = pd.DataFrame(columns=['drug1', 'interaction', 'drug2'])\n",
    "    \n",
    "    for inter in interactions:\n",
    "        train, valid, test = split_data_relation(ddi_df[ddi_df['interaction'] == inter])\n",
    "        train_triplets = pd.concat([train_triplets, train])\n",
    "        valid_triplets = pd.concat([valid_triplets, valid])\n",
    "        test_triplets = pd.concat([test_triplets, test])\n",
    "        \n",
    "    train_triplets.rename(columns={'drug1': 'head', 'interaction': 'relation', 'drug2': 'tail'}, inplace=True)\n",
    "    valid_triplets.rename(columns={'drug1': 'head', 'interaction': 'relation', 'drug2': 'tail'}, inplace=True)\n",
    "    test_triplets.rename(columns={'drug1': 'head', 'interaction': 'relation', 'drug2': 'tail'}, inplace=True)    \n",
    "\n",
    "\n",
    "    print('train dataset size:', train_triplets.shape[0])\n",
    "    print('validation dataset size:',valid_triplets.shape[0])\n",
    "    print('test dataset size:',test_triplets.shape[0])\n",
    "    \n",
    "    return train_triplets, valid_triplets, test_triplets    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DrugBank drug-drug interactions\n",
    "print('DrugBank drug-drug interactions')\n",
    "train_triplets_ddi, valid_triplets_ddi, test_triplets_ddi = split_ddi_dataset(ddi_df)\n",
    "\n",
    "print()\n",
    "\n",
    "# Drug Supplement database - drug-suplement interactions\n",
    "print('Drug Supplement database - drug-suplement interactions')\n",
    "train_triplets_ds, valid_triplets_ds, test_triplets_ds = split_drug_supplements_dataset(drug_supplement_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all interactions\n",
    "train_triplets = pd.concat([train_triplets_ddi, train_triplets_ds])\n",
    "valid_triplets = pd.concat([valid_triplets_ddi, valid_triplets_ds])\n",
    "test_triplets = pd.concat([test_triplets_ddi, test_triplets_ds])\n",
    "\n",
    "print('All interactions:')\n",
    "print('train dataset size:', train_triplets.shape[0])\n",
    "print('validation dataset size:',valid_triplets.shape[0])\n",
    "print('test dataset size:',test_triplets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relations = set(train_triplets.relation)\n",
    "print('Number of unique interactions:', len(all_relations))\n",
    "print(list(all_relations)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add rest of the data into the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = listdir(data_dir)\n",
    "\n",
    "for file in files:\n",
    "    if file == 'ddi.tsv' or file == '.ipynb_checkpoints' or file == 'ds_relations.tsv':\n",
    "        continue\n",
    "    if 'train' in file or 'valid' in file or 'test' in file:\n",
    "        continue\n",
    "           \n",
    "    df = pd.read_csv(data_dir + file, sep='\\t', index_col=[0])\n",
    "    \n",
    "    # if file == 'ds_relations.tsv':\n",
    "    #     df = df[df['REL'] == 'has_ingredient']\n",
    "    \n",
    "    df.set_axis(['head', 'relation', 'tail'], axis=1, inplace=True) \n",
    "    train_triplets = pd.concat([train_triplets, df])\n",
    "    \n",
    "print('Final size of train dataset (with other relations):', train_triplets.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = train_triplets.astype(str)\n",
    "valid_triplets = valid_triplets.astype(str)\n",
    "test_triplets = test_triplets.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train, valid and test datasets\n",
    "\n",
    "train_triplets.to_csv(data_dir + 'train.tsv', sep='\\t')\n",
    "valid_triplets.to_csv(data_dir + 'valid.tsv', sep='\\t')\n",
    "test_triplets.to_csv(data_dir + 'test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "specification = '_with_biokg'\n",
    "\n",
    "train_triplets = pd.read_csv(data_dir + 'train' + specification + '.tsv', sep='\\t')\n",
    "valid_triplets = pd.read_csv(data_dir + 'valid' + specification + '.tsv', sep='\\t')\n",
    "test_triplets = pd.read_csv(data_dir + 'test' + specification + '.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_drugs = pd.read_csv('../data/common_drugs.csv', sep=';')\n",
    "common_drugs = common_drugs['DrugBank_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets[train_triplets['head'] == common_drugs[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert datasets into Triples Factory format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_triples_factory(data):\n",
    "    tf_data = TriplesFactory.from_labeled_triples(\n",
    "      data[[\"head\", \"relation\", \"tail\"]].values,\n",
    "      create_inverse_triples=False,\n",
    "      entity_to_id=None,\n",
    "      relation_to_id=None,\n",
    "      compact_id=False \n",
    "    )\n",
    "    print(tf_data)  \n",
    "    return tf_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriplesFactory(num_entities=33046, num_relations=58, create_inverse_triples=False, num_triples=2074440)\n",
      "TriplesFactory(num_entities=2402, num_relations=4, create_inverse_triples=False, num_triples=164903)\n",
      "TriplesFactory(num_entities=2415, num_relations=4, create_inverse_triples=False, num_triples=164926)\n"
     ]
    }
   ],
   "source": [
    "tf_train = convert_to_triples_factory(train_triplets.astype(str))\n",
    "tf_valid = convert_to_triples_factory(valid_triplets.astype(str))\n",
    "tf_test = convert_to_triples_factory(test_triplets.astype(str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=2378562106\n",
      "using automatically assigned random_state=481061183\n",
      "using automatically assigned random_state=770684229\n"
     ]
    }
   ],
   "source": [
    "# take just subset of data for testing\n",
    "\n",
    "train_sub, _ = tf_train.split(0.15)\n",
    "valid_sub, _ = tf_valid.split(0.15)\n",
    "test_sub, _ = tf_test.split(0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'epochs': 2, \n",
    "          'optimizer': 'adam', \n",
    "          'learning_rate': 0.005,\n",
    "          'loss': 'MarginRankingLoss',\n",
    "          'batch': 512,\n",
    "          'embedding_dim': 300,\n",
    "          'margin': 0.64,\n",
    "          'evaluator': 'ogb'\n",
    "         }\n",
    "params['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. Setting to 784705745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'evaluation_factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_kg\u001b[38;5;241m.\u001b[39mset_params2(params)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel_kg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/models/kg_model.py:80\u001b[0m, in \u001b[0;36mKG_model.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_sampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_sampler_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_negs_per_pos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_neg_per_pos\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecification\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_checkpoint.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkg_checkpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pykeen/pipeline/api.py:1539\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001b[0m\n\u001b[1;32m   1509\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m _handle_model(\n\u001b[1;32m   1510\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m   1511\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1522\u001b[0m     regularizer_kwargs\u001b[38;5;241m=\u001b[39mregularizer_kwargs,\n\u001b[1;32m   1523\u001b[0m )\n\u001b[1;32m   1525\u001b[0m training_loop_instance \u001b[38;5;241m=\u001b[39m _handle_training_loop(\n\u001b[1;32m   1526\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[1;32m   1527\u001b[0m     model_instance\u001b[38;5;241m=\u001b[39mmodel_instance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     negative_sampler_kwargs\u001b[38;5;241m=\u001b[39mnegative_sampler_kwargs,\n\u001b[1;32m   1537\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m evaluator_instance, evaluation_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_handle_evaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_result_tracker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_result_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m stopper_instance, configuration, losses, train_seconds \u001b[38;5;241m=\u001b[39m _handle_training(\n\u001b[1;32m   1547\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[1;32m   1548\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     use_tqdm\u001b[38;5;241m=\u001b[39muse_tqdm,\n\u001b[1;32m   1560\u001b[0m )\n\u001b[1;32m   1562\u001b[0m metric_results, evaluate_seconds \u001b[38;5;241m=\u001b[39m _handle_evaluation(\n\u001b[1;32m   1563\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[1;32m   1564\u001b[0m     model_instance\u001b[38;5;241m=\u001b[39mmodel_instance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     use_tqdm\u001b[38;5;241m=\u001b[39muse_tqdm,\n\u001b[1;32m   1576\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pykeen/pipeline/api.py:1118\u001b[0m, in \u001b[0;36m_handle_evaluator\u001b[0;34m(_result_tracker, evaluator, evaluator_kwargs, evaluation_kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     evaluator_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1117\u001b[0m evaluator_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(evaluator_kwargs)\n\u001b[0;32m-> 1118\u001b[0m evaluator_instance: Evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m _result_tracker\u001b[38;5;241m.\u001b[39mlog_params(\n\u001b[1;32m   1120\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1121\u001b[0m         evaluator\u001b[38;5;241m=\u001b[39mevaluator_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1122\u001b[0m         evaluator_kwargs\u001b[38;5;241m=\u001b[39mevaluator_kwargs,\n\u001b[1;32m   1123\u001b[0m     ),\n\u001b[1;32m   1124\u001b[0m )\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluation_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/class_resolver/api.py:210\u001b[0m, in \u001b[0;36mClassResolver.make\u001b[0;34m(self, query, pos_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(text \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m MISSING_ARGS):\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedKeywordError(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# An instance was passed, and it will go through without modification.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/class_resolver/api.py:204\u001b[0m, in \u001b[0;36mClassResolver.make\u001b[0;34m(self, query, pos_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mcls\u001b[39m: Type[X] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup(query)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpos_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired keyword-only argument\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pykeen/evaluation/ogb_evaluator.py:35\u001b[0m, in \u001b[0;36mOGBEvaluator.__init__\u001b[0;34m(self, filtered, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtered:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOGB evaluator is already filtered, but not dynamically like other evaluators because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit requires pre-calculated filtered negative triples. Therefore, it is not allowed to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept filtered=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiltered\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'evaluation_factory'"
     ]
    }
   ],
   "source": [
    "model_kg = KG_model('complex', tf_train, tf_valid, tf_test, 'jupyter_test')\n",
    "model_kg.set_params2(params)\n",
    "\n",
    "print('Training...')\n",
    "model_kg.train()\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kg.trained_model.metric_results.to_df()\n",
    "\n",
    "head = common_drugs[0]\n",
    "relation = 'negative'\n",
    "\n",
    "pred = predict_target(\n",
    "            model = model_kg.trained_model.model, \n",
    "            head = head, \n",
    "            relation = relation, \n",
    "            triples_factory = model_kg.trained_model.training,\n",
    "        )\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kg.trained_model.get_metric('mrr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model\n",
    "result = pipeline(\n",
    "    training=train_sub,\n",
    "    testing=test_sub,\n",
    "    validation=valid_sub,\n",
    "    model='RGCN',\n",
    "    epochs=1,\n",
    "#     evaluator=RankBasedEvaluator,\n",
    "    model_kwargs=dict(embedding_dim=20, num_layers=1, interaction='distmult'),\n",
    "#     device='gpu',\n",
    "#     optimizer='Adam',\n",
    "#     training_kwargs=dict(\n",
    "#         batch_size=32,\n",
    "#         use_tqdm=True\n",
    "# #         num_epochs=2,\n",
    "# #         checkpoint_name='transE_checkpoint.pt',\n",
    "# #         checkpoint_directory='kg_ckeckpoints',\n",
    "# #         checkpoint_frequency=0\n",
    "#     ),\n",
    "#     use_tqdm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_10 = result.get_metric('hits@10')\n",
    "hits_at_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metric_results.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.save_to_directory(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hetionet(num_entities=45158, num_relations=24, create_inverse_triples=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykeen.datasets import Hetionet, BioKG\n",
    "dataset = Hetionet()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.training.entity_id_to_label.values()\n",
    "for l in labels:\n",
    "    if 'Compound' in l:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "OGB evaluator is already filtered, but not dynamically like other evaluators because it requires pre-calculated filtered negative triples. Therefore, it is not allowed to accept filtered=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     dataset=BioKG,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     training\u001b[38;5;241m=\u001b[39mtf_train,\n\u001b[1;32m      4\u001b[0m     vlidation\u001b[38;5;241m=\u001b[39mtf_valid,\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplEx\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m----> 7\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39m\u001b[43mOGBEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_valid\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      8\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     10\u001b[0m     },\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     optimizer_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     13\u001b[0m                 lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     14\u001b[0m     ),\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarginRankingLoss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pykeen/evaluation/ogb_evaluator.py:30\u001b[0m, in \u001b[0;36mOGBEvaluator.__init__\u001b[0;34m(self, filtered, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filtered: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filtered:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOGB evaluator is already filtered, but not dynamically like other evaluators because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit requires pre-calculated filtered negative triples. Therefore, it is not allowed to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept filtered=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         )\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, filtered\u001b[38;5;241m=\u001b[39mfiltered)\n",
      "\u001b[0;31mValueError\u001b[0m: OGB evaluator is already filtered, but not dynamically like other evaluators because it requires pre-calculated filtered negative triples. Therefore, it is not allowed to accept filtered=True"
     ]
    }
   ],
   "source": [
    "result = pipeline(\n",
    "#     dataset=BioKG,\n",
    "    training=tf_train,\n",
    "    vlidation=tf_valid,\n",
    "    model='ComplEx',\n",
    "    epochs=5,\n",
    "    evaluator=OGBEvaluator(tf_test),\n",
    "    model_kwargs={\n",
    "        'embedding_dim': 1000\n",
    "    },\n",
    "    optimizer='Adam',\n",
    "    optimizer_kwargs = dict(\n",
    "                lr = 0.001\n",
    "    ),\n",
    "    loss = 'MarginRankingLoss',\n",
    "    device='gpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TriplesFactory(num_entities=2415, num_relations=4, create_inverse_triples=False, num_triples=164926)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = result.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot_losses()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metric_results.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get_metric('mrr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.hpo import hpo_pipeline_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'optuna': dict(\n",
    "        n_trials=5,\n",
    "    ),\n",
    "    'pipeline': dict(\n",
    "        training = tf_train,\n",
    "        validation = tf_valid,\n",
    "        testing = tf_test,\n",
    "        model='TransR',\n",
    "        model_kwargs_ranges=dict(\n",
    "               embedding_dim=dict(type=int, low=20, high=160, q=20),\n",
    "        ),\n",
    "        optimizer='Adam',\n",
    "        optimizer_kwargs=dict(lr=0.01),\n",
    "        loss='marginranking',\n",
    "        loss_kwargs=dict(margin=1),\n",
    "        training_loop='slcwa',\n",
    "        training_kwargs=dict(num_epochs=100, batch_size=128),\n",
    "        negative_sampler='basic',\n",
    "        negative_sampler_kwargs=dict(num_negs_per_pos=1),\n",
    "        evaluator_kwargs=dict(filtered=True),\n",
    "        evaluation_kwargs=dict(batch_size=128),\n",
    "        stopper='early',\n",
    "        stopper_kwargs=dict(frequency=5, patience=2, relative_delta=0.002),\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_pipeline_result = hpo_pipeline_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_pipeline_result.save_to_directory('hpo_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {\n",
    "    \"metadata\": {\n",
    "    \"title\": \"nations - try\",\n",
    "    \"comments\": \"comment\"\n",
    "  },\n",
    "  \"pipeline\": {\n",
    "    \"dataset\": \"nations\",\n",
    "    \"model\": \"TransE\",\n",
    "    \"model_kwargs\": {\n",
    "      \"embedding_dim\": 50,\n",
    "      \"scoring_fct_norm\": 1\n",
    "    },\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"optimizer_kwargs\": {\n",
    "      \"lr\": 0.01\n",
    "    },\n",
    "    \"loss\": \"MarginRankingLoss\",\n",
    "    \"loss_kwargs\": {\n",
    "      \"reduction\": \"mean\",\n",
    "      \"margin\": 1\n",
    "    },\n",
    "    \"training_loop\": \"slcwa\",\n",
    "    \"negative_sampler\": \"basic\",\n",
    "    \"negative_sampler_kwargs\": {\n",
    "      \"num_negs_per_pos\": 1\n",
    "    },\n",
    "    \"training_kwargs\": {\n",
    "      \"num_epochs\": 100,\n",
    "      \"batch_size\": 32\n",
    "    },\n",
    "    \"evaluator_kwargs\": {\n",
    "      \"filtered\": True\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_result = pipeline_from_config(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.metric_results.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.datasets import get_dataset\n",
    "from pykeen.trackers import ResultTracker\n",
    "\n",
    "\n",
    "dataset = get_dataset(dataset=\"nations\")\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    model='RotatE',\n",
    "    dataset=dataset,\n",
    "    result_tracker=\"console\",\n",
    "    result_tracker_kwargs = dict(metric_filter='.*head.realistic.hits_at_10.*'),\n",
    "    training_kwargs = dict(\n",
    "        num_epochs = 5,\n",
    "        callbacks=\"validation-loss\",\n",
    "        callback_kwargs=dict(\n",
    "            evaluation_triples=dataset.validation.mapped_triples,\n",
    "            prefix=\"validation\",\n",
    "        ),\n",
    "    )    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8343f1c295490ad08729f17064e1ab8ac071c711efe2732632787d24e0261b0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating my KG on OGB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from kg_model import KG_model\n",
    "\n",
    "from ogb.linkproppred import Evaluator, PygLinkPropPredDataset\n",
    "\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "from pykeen.triples import CoreTriplesFactory\n",
    "from pykeen.pipeline import pipeline, pipeline_from_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/dataset-ogb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi', root=dataset_dir, transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "data.adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_edge = dataset.get_edge_split()\n",
    "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n",
    "train_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_edge['edge_neg'].shape)\n",
    "print(valid_edge['edge'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_triples_factory(data, num_entities, num_relations):\n",
    "#     tf_data = TriplesFactory.from_labeled_triples(\n",
    "#         data[[\"head\", \"relation\", \"tail\"]].values,\n",
    "#         create_inverse_triples=False,\n",
    "#         entity_to_id=None,\n",
    "#         relation_to_id=None,\n",
    "#         compact_id=False \n",
    "#     )\n",
    "    \n",
    "    tf_data = CoreTriplesFactory(\n",
    "        data,\n",
    "        num_entities = num_entities,\n",
    "        num_relations = num_relations\n",
    "    )\n",
    "\n",
    "    print(tf_data)\n",
    "\n",
    "    return tf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add relation type - interacts with\n",
    "\n",
    "train = train_edge['edge']\n",
    "train = torch.tensor([[x[0], 0, x[1]] for x in train])\n",
    "train_df = pd.DataFrame(train, columns=['head', 'relation', 'tail'])\n",
    "\n",
    "valid = valid_edge['edge']\n",
    "valid = torch.tensor([[x[0], 0, x[1]] for x in valid])\n",
    "valid_df = pd.DataFrame(valid, columns=['head', 'relation', 'tail'])\n",
    "\n",
    "valid_neg = valid_edge['edge_neg']\n",
    "valid_neg = torch.tensor([[x[0], 0, x[1]] for x in valid_neg])\n",
    "\n",
    "test = test_edge['edge']\n",
    "test = torch.tensor([[x[0], 0, x[1]] for x in test])\n",
    "test_df = pd.DataFrame(test, columns=['head', 'relation', 'tail'])\n",
    "\n",
    "test_neg = test_edge['edge_neg']\n",
    "test_neg = torch.tensor([[x[0], 0, x[1]] for x in test_neg])\n",
    "\n",
    "num_entities = data.num_nodes\n",
    "\n",
    "train_tf = convert_to_triples_factory(torch.tensor(train_df.values), num_entities, 1)\n",
    "valid_tf = convert_to_triples_factory(torch.tensor(valid_df.values), num_entities, 1)\n",
    "test_tf = convert_to_triples_factory(torch.tensor(test_df.values), num_entities, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset split to txt files\n",
    "\n",
    "dir_data_my_split = 'dataset/ogbl_ddi-my_split/'\n",
    "\n",
    "train_df.to_csv(dir_data_my_split + 'train.txt', sep='\\t', header=False, index=False)\n",
    "valid_df.to_csv(dir_data_my_split + 'valid.txt', sep='\\t', header=False, index=False)\n",
    "test_df.to_csv(dir_data_my_split + 'test.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train my KG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_kg = KG_model('transe', train_tf, valid_tf, test_tf, 'ogb-ddi')\n",
    "# model_kg.set_params(10, 'Adam', RankBasedEvaluator, 'gpu')\n",
    "# print('Training...')\n",
    "# model_kg.train()\n",
    "# print('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_result = model_kg.trained_model\n",
    "# pipeline_result.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'metadata': dict(\n",
    "            title='ComplEx'\n",
    "        ),\n",
    "        'pipeline': dict(\n",
    "            training = '../data/dataset-ogb/ogbl_ddi-my_split/train.txt',\n",
    "            validation = '../data/dataset-ogb/ogbl_ddi-my_split/valid.txt',\n",
    "            testing = '../data/dataset-ogb/ogbl_ddi-my_split/test.txt',\n",
    "            model='ComplEx',\n",
    "            model_kwargs=dict(\n",
    "                   embedding_dim=1000,\n",
    "            ),\n",
    "            optimizer='Adam',\n",
    "            optimizer_kwargs=dict(lr=0.001),\n",
    "            loss='marginranking',\n",
    "            loss_kwargs=dict(margin=2.64),\n",
    "            training_loop='slcwa',\n",
    "            training_kwargs=dict(\n",
    "                num_epochs=20, \n",
    "                batch_size=512, \n",
    "                checkpoint_name='Complex_checkpoint-ogb-ddi.pt',\n",
    "                checkpoint_directory='kg_checkpoints',\n",
    "                checkpoint_frequency=5    \n",
    "            ),\n",
    "            device='gpu',\n",
    "            negative_sampler='basic',\n",
    "            negative_sampler_kwargs=dict(num_negs_per_pos=94),\n",
    "            evaluator='rankbased',\n",
    "            evaluator_kwargs=dict(filtered=True),\n",
    "            evaluation_kwargs=dict(batch_size=64),\n",
    "            stopper='early',\n",
    "            stopper_kwargs=dict(\n",
    "                patience=10,\n",
    "                relative_delta=0.002\n",
    "            )\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = pipeline_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scores for given triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute scores for positive and negative triplets \n",
    "batch_size = 512\n",
    "\n",
    "n = train.size(0) // batch_size\n",
    "pos_train_preds = []\n",
    "for i in range(n+1):\n",
    "    start_idx = i*batch_size\n",
    "    end_idx = min((i+1)*batch_size, train.size(0))\n",
    "    edge = train[start_idx:end_idx]\n",
    "    pos_train_preds += [pipeline_result.model.score_hrt(edge).squeeze().cpu().detach()]\n",
    "pos_train_pred = torch.cat(pos_train_preds, dim=0)\n",
    "\n",
    "n = valid.size(0) // batch_size\n",
    "pos_valid_preds = []\n",
    "for i in range(n+1):\n",
    "    start_idx = i*batch_size\n",
    "    end_idx = min((i+1)*batch_size, valid.size(0))\n",
    "    edge = valid[start_idx:end_idx]\n",
    "    pos_valid_preds += [pipeline_result.model.score_hrt(edge).squeeze().cpu().detach()]\n",
    "pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
    "\n",
    "n = valid_neg.size(0) // batch_size\n",
    "neg_valid_preds = []\n",
    "for i in range(n+1):\n",
    "    start_idx = i*batch_size\n",
    "    end_idx = min((i+1)*batch_size, valid_neg.size(0))\n",
    "    edge = valid_neg[start_idx:end_idx]\n",
    "    neg_valid_preds += [pipeline_result.model.score_hrt(edge).squeeze().cpu().detach()]\n",
    "neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
    "\n",
    "n = test.size(0) // batch_size\n",
    "pos_test_preds = []\n",
    "for i in range(n+1):\n",
    "    start_idx = i*batch_size\n",
    "    end_idx = min((i+1)*batch_size, test.size(0))\n",
    "    edge = test[start_idx:end_idx]\n",
    "    pos_test_preds += [pipeline_result.model.score_hrt(edge).squeeze().cpu().detach()]\n",
    "pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
    "\n",
    "n = test_neg.size(0) // batch_size\n",
    "neg_test_preds = []\n",
    "for i in range(n+1):\n",
    "    start_idx = i*batch_size\n",
    "    end_idx = min((i+1)*batch_size, test_neg.size(0))\n",
    "    edge = test_neg[start_idx:end_idx]\n",
    "    neg_test_preds += [pipeline_result.model.score_hrt(edge).squeeze().cpu().detach()]\n",
    "neg_test_pred = torch.cat(neg_test_preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score ranges train: {torch.min(pos_train_pred)} - {torch.max(pos_train_pred)}')\n",
    "print(f'Score ranges valid: {torch.min(pos_valid_pred)} - {torch.max(pos_valid_pred)}')\n",
    "print(f'Score ranges valid neg: {torch.min(neg_valid_pred)} - {torch.max(neg_valid_pred)}')\n",
    "print(f'Score ranges test: {torch.min(pos_test_pred)} - {torch.max(pos_test_pred)}')\n",
    "print(f'Score ranges test neg: {torch.min(neg_test_pred)} - {torch.max(neg_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate my results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the coputed scores - hits@K\n",
    "\n",
    "evaluator = Evaluator(name = 'ogbl-ddi')\n",
    "\n",
    "results = {}\n",
    "for K in [10, 20, 30]:\n",
    "    evaluator.K = K\n",
    "    train_hits = evaluator.eval({\n",
    "        'y_pred_pos': pos_train_pred,\n",
    "        'y_pred_neg': neg_valid_pred,\n",
    "    })[f'hits@{K}']\n",
    "    valid_hits = evaluator.eval({\n",
    "        'y_pred_pos': pos_valid_pred,\n",
    "        'y_pred_neg': neg_valid_pred,\n",
    "    })[f'hits@{K}']\n",
    "    test_hits = evaluator.eval({\n",
    "        'y_pred_pos': pos_test_pred,\n",
    "        'y_pred_neg': neg_test_pred,\n",
    "    })[f'hits@{K}']\n",
    "    \n",
    "    results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n",
    "    \n",
    "    \n",
    "for hits, result in results.items():\n",
    "    print(hits)\n",
    "#     print(result)\n",
    "    train_hits, valid_hits, test_hits = result\n",
    "    print(f'Train: {100 * train_hits:.2f}%')\n",
    "    print(f'Valid: {100 * valid_hits:.2f}%')\n",
    "    print(f'Test: {100 * test_hits:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_kg.trained_model.get_metric('hits@1'))\n",
    "print(model_kg.trained_model.get_metric('hits@5'))\n",
    "print(model_kg.trained_model.get_metric('hits@10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_kg.trained_model.get_metric('mrr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioKG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygLinkPropPredDataset(name='ogbl-biokg', root=dataset_dir, transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_edge = dataset.get_edge_split()\n",
    "train_triples, valid_triples, test_triples = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(max(train_triples['relation']))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_triples (valid/test):\n",
    "* head type (e.g. disease)\n",
    "* head - tensor\n",
    "* relation - tensor\n",
    "* tail type (e.g. protein)\n",
    "* tail - tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_name_id = data['edge_index_dict'].keys()\n",
    "head = train_triples['head']\n",
    "relation = train_triples['relation']\n",
    "tail = train_triples['tail']\n",
    "train_df = pd.DataFrame({'head': head, 'relation': relation, 'tail': tail})\n",
    "print(train_df.head())\n",
    "\n",
    "head = valid_triples['head']\n",
    "relation = valid_triples['relation']\n",
    "tail = valid_triples['tail']\n",
    "valid_df = pd.DataFrame({'head': head, 'relation': relation, 'tail': tail})\n",
    "\n",
    "\n",
    "head = test_triples['head']\n",
    "relation = test_triples['relation']\n",
    "tail = test_triples['tail']\n",
    "test_df = pd.DataFrame({'head': head, 'relation': relation, 'tail': tail})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entities = sum(dataset[0]['num_nodes_dict'].values())\n",
    "num_relations = int(max(train_triples['relation']))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = convert_to_triples_factory(torch.tensor(train_df.values), num_entities, num_relations)\n",
    "valid_tf = convert_to_triples_factory(torch.tensor(valid_df.values), num_entities, num_relations)\n",
    "test_tf = convert_to_triples_factory(torch.tensor(test_df.values), num_entities, num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_my_split = dataset_dir + 'ogbl_biokg-my_split/'\n",
    "\n",
    "train_df.to_csv(dir_data_my_split + 'train.txt', sep='\\t', header=False, index=False)\n",
    "valid_df.to_csv(dir_data_my_split + 'valid.txt', sep='\\t', header=False, index=False)\n",
    "test_df.to_csv(dir_data_my_split + 'test.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.pipeline.api:=> no training loop checkpoint file found at 'kg_checkpoints/ComplEx-ogb-biokg_checkpoint.pt'. Creating a new file.\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 1341638575.\n",
      "INFO:pykeen.pipeline.api:Using device: gpu\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 7.79 GiB total capacity; 360.64 MiB already allocated; 261.25 MiB free; 386.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplEx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m specification \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mogb-biokg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/dataset-ogb/ogbl_biokg-my_split/train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/dataset-ogb/ogbl_biokg-my_split/valid.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/dataset-ogb/ogbl_biokg-my_split/test.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMarginRankingLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrankbased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspecification\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_checkpoint.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheckpoint_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkg_checkpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m  \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pykeen/pipeline/api.py:1509\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001b[0m\n\u001b[1;32m   1496\u001b[0m _result_tracker\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39mtitle)\n\u001b[1;32m   1498\u001b[0m training, testing, validation \u001b[38;5;241m=\u001b[39m _handle_dataset(\n\u001b[1;32m   1499\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[1;32m   1500\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     evaluation_relation_whitelist\u001b[38;5;241m=\u001b[39mevaluation_relation_whitelist,\n\u001b[1;32m   1507\u001b[0m )\n\u001b[0;32m-> 1509\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[43m_handle_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_result_tracker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_result_tracker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_random_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_random_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minteraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteraction_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minteraction_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1525\u001b[0m training_loop_instance \u001b[38;5;241m=\u001b[39m _handle_training_loop(\n\u001b[1;32m   1526\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[1;32m   1527\u001b[0m     model_instance\u001b[38;5;241m=\u001b[39mmodel_instance,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     negative_sampler_kwargs\u001b[38;5;241m=\u001b[39mnegative_sampler_kwargs,\n\u001b[1;32m   1537\u001b[0m )\n\u001b[1;32m   1539\u001b[0m evaluator_instance, evaluation_kwargs \u001b[38;5;241m=\u001b[39m _handle_evaluator(\n\u001b[1;32m   1540\u001b[0m     _result_tracker\u001b[38;5;241m=\u001b[39m_result_tracker,\n\u001b[1;32m   1541\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[1;32m   1542\u001b[0m     evaluator_kwargs\u001b[38;5;241m=\u001b[39mevaluator_kwargs,\n\u001b[1;32m   1543\u001b[0m     evaluation_kwargs\u001b[38;5;241m=\u001b[39mevaluation_kwargs,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pykeen/pipeline/api.py:991\u001b[0m, in \u001b[0;36m_handle_model\u001b[0;34m(device, _result_tracker, _random_seed, training, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# TODO should training be reset?\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# TODO should kwargs for loss and regularizer be checked and raised for?\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     model_instance, model_kwargs \u001b[38;5;241m=\u001b[39m _build_model_helper(\n\u001b[1;32m    980\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    981\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m         training_triples_factory\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m    989\u001b[0m     )\n\u001b[0;32m--> 991\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# Log model parameters\u001b[39;00m\n\u001b[1;32m    994\u001b[0m _result_tracker\u001b[38;5;241m.\u001b[39mlog_params(\n\u001b[1;32m    995\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    996\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    997\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    998\u001b[0m     ),\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 7.79 GiB total capacity; 360.64 MiB already allocated; 261.25 MiB free; 386.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_name = 'ComplEx'\n",
    "specification = 'ogb-biokg'\n",
    "\n",
    "pipeline = pipeline(\n",
    "            training = '../data/dataset-ogb/ogbl_biokg-my_split/train.txt',\n",
    "            validation = '../data/dataset-ogb/ogbl_biokg-my_split/valid.txt',\n",
    "            testing = '../data/dataset-ogb/ogbl_biokg-my_split/test.txt',\n",
    "            model = model_name,\n",
    "            model_kwargs = dict(\n",
    "                embedding_dim = 1000\n",
    "            ),\n",
    "            loss = 'MarginRankingLoss',\n",
    "            optimizer = 'Adam',\n",
    "            optimizer_kwargs = dict(\n",
    "                lr = 0.001\n",
    "            ),\n",
    "            evaluator = 'rankbased',\n",
    "            device = 'gpu',\n",
    "            training_kwargs = dict(\n",
    "                batch_size = 100,\n",
    "                num_epochs = 5,\n",
    "                checkpoint_name = model_name + '-' + specification + '_checkpoint.pt',\n",
    "                checkpoint_directory = 'kg_checkpoints'\n",
    "            ),\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.metric_results.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_metric('mrr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def test_step(trained_model, test_triplets, num_entities, num_relations):   \n",
    "    evaluator = Evaluator(name = 'ogbl-biokg')\n",
    "    \n",
    "    batch_size = 512\n",
    "    \n",
    "    test_logs = defaultdict(list)\n",
    "    \n",
    "    n = test_triplets['head'].size(0) // batch_size\n",
    "    test_dataset = []\n",
    "    for i in range(n+1):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = min((i+1)*batch_size, test_triplets['head'].size(0))\n",
    "        positive_triples = torch.stack([test_triples['head'][start_idx:end_idx], \n",
    "                                        test_triples['relation'][start_idx:end_idx], \n",
    "                                        test_triples['tail'][start_idx:end_idx]], 1)\n",
    "        \n",
    "        num_neg = test_triples['head_neg'][start_idx:end_idx].size(0)\n",
    "        neg_nodes = test_triples['head_neg'].shape[1]\n",
    "        \n",
    "#         print(num_neg, neg_nodes)\n",
    "        negative_triples = torch.stack([test_triples['head_neg'][start_idx:end_idx], \n",
    "                                        torch.randint(0, num_relations, (num_neg, neg_nodes)), \n",
    "                                        test_triples['tail_neg'][start_idx:end_idx]], 1)\n",
    "#         negative_triples = torch.stack([torch.randint(0, num_entities, (num_neg,)), \n",
    "#                                         torch.randint(0, num_relations, (num_neg,)), \n",
    "#                                         torch.randint(0,  num_entities, (num_neg,))], 1)\n",
    "        test_dataset.append((positive_triples, negative_triples))\n",
    "\n",
    "#     step = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#     for test_dataset in test_dataset_list:\n",
    "        for positive_sample, negative_sample in test_dataset:\n",
    "    #             if args.cuda:\n",
    "            positive_sample = positive_sample.cuda()\n",
    "            negative_sample = negative_sample.cuda()\n",
    "\n",
    "    #             score = model((positive_sample, negative_sample), mode)\n",
    "            score = trained_model.model.score_hrt(positive_sample)\n",
    "            score_neg = trained_model.model.score_hrt(negative_sample)\n",
    "\n",
    "\n",
    "            batch_results = evaluator.eval({'y_pred_pos': score[:, 0], \n",
    "                                        'y_pred_neg': score_neg.squeeze(-1)})\n",
    "            for metric in batch_results:\n",
    "                test_logs[metric].append(batch_results[metric])\n",
    "\n",
    "\n",
    "#             print('Evaluating the model... (%d)' % (step))\n",
    "\n",
    "#             step += 1\n",
    "\n",
    "    metrics = {}\n",
    "    for metric in test_logs:\n",
    "        metrics[metric] = torch.cat(test_logs[metric]).mean().item()\n",
    "        \n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = test_step(pipeline, valid_triples, num_entities, num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8343f1c295490ad08729f17064e1ab8ac071c711efe2732632787d24e0261b0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
